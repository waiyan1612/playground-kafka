# KTable Demo

## Scenario

1. Run SpringBoot app.
2. Run [KTableDemo](KTableDemo.java). There will be some warnings / errors because the topics have not been created yet.
3. Produce Txn and Pay records using [TxnNPayProducer](TxnNPayProducer.java).
4. You should start to see the records in `KTableDemo` console.
   - `TRANSACTION_WHERE_FIRST_PAY_WILL_FAIL_AND_SECOND_WILL_PASS` should show the latest `PAYMENT_1` - which is `ACCEPTED` 
   - `TRANSACTION_WHERE_FIRST_PAY_WILL_MISS_THE_WINDOW` should show `PAYMENT_2`
   - `TRANSACTION_WHERE_PAYMENT_WILL_NVR_BE_MADE` should show `null` for the payment since there is no payment related data for this.
   
5. Run [PayDeleteProducer](PayDeleteProducer.java).
6. Observe that `TRANSACTION_WHERE_FIRST_PAY_WILL_FAIL_AND_SECOND_WILL_PASS` no longer shows `PAYMENT_1` information.
7. Run [TxnDeleteProducer](TxnDeleteProducer.java).
8. Observe that `TRANSACTION_WHERE_FIRST_PAY_WILL_FAIL_AND_SECOND_WILL_PASS` is no longer present in the KTable since both sides of the join no longer have the data.

## Readings

- [Medium blog - Kafka Streams — Optimizing RocksDB](https://verticalserve.medium.com/kafka-streams-optimizing-rocksdb-99a6cc14bc93)
- [Medium blog - Kafka-Streams and rocksdb in the space-time continuum and a little bit of configuration](https://blog.dy.engineering/kafka-streams-and-rocksdb-in-the-space-time-continuum-and-a-little-bit-of-configuration-40edb5ee9ed7)
- [Confluent - When do KTable records expire if you don’t tombstone them](https://forum.confluent.io/t/when-do-ktable-records-expire-if-you-dont-tombstone-them/6967)
- [How to expire KTable rows based on TTL in Kafka Streams](https://developer.confluent.io/confluent-tutorials/schedule-ktable-ttl/kstreams/).

### How to expire KTable rows based on TTL in Kafka Streams

This is an archive of [How to expire KTable rows based on TTL in Kafka Streams](https://developer.confluent.io/confluent-tutorials/schedule-ktable-ttl/kstreams/). 

> If you have a Kafka Streams application or ksqlDB application which uses KTables from a topic in Kafka, you can purge older data by making clever use of tombstones. Specifically, by maintaining a state store containing TTLs, you can write tombstones out to the topic underlying the KTable in order to expire data.
> 
> The tombstones can be generated by first using the Kafka Streams Processor API to maintain the latest stream processing time per key:

```java
 table.toStream()
     .process(new TTLEmitter<String, String, String, String>(MAX_AGE,
             SCAN_FREQUENCY, STATE_STORE_NAME), STATE_STORE_NAME)
     .to(INPUT_TOPIC_TABLE, Produced.with(Serdes.String(), Serdes.String()));
```
> The TTLEmitter class implements the ProcessorSupplier interface with the method to process each record looking like this. Note that because the processor will also receive tombstones, we use tombstones to clean out TTLs from the state store:
```java
 @Override
 public void process(Record<Kin, Vin> record) {
     // this gets invoked for each new record we consume. If it's a tombstone, delete
     // it from our state store. Otherwise, store the record timestamp.
     if (record.value() == null) {
         System.out.println("CLEANING key=" + record.key());
         stateStore.delete((Kout) record.key());
     } else {
         System.out.println("UPDATING key=" + record.key());
         stateStore.put((Kout) record.key(), context.currentStreamTimeMs());
     }
 }
```

> The final piece of the puzzle is to use ProcessorContext.schedule to periodically pass over the state store and emit tombstones for any records that are older than a specified cutoff:
```java
context.schedule(scanFrequency, PunctuationType.STREAM_TIME, timestamp -> {
  final long cutoff = timestamp - maxAge.toMillis();

  try (final KeyValueIterator<Kout, Long> all = stateStore.all()) {
      while (all.hasNext()) {
          final KeyValue<Kout, Long> record = all.next();
          if (record.value != null && record.value < cutoff) {
              // record's last update was older than the cutoff, so emit a tombstone.
              context.forward(new Record(record.key, null, 0, null));
          }
      }
  }
});
```

